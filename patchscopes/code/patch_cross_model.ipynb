{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from torch import cuda\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-6.9b\": set_hs_patch_hooks_neox,\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4479f16b9a544b79bb8790693701d8de"
     ]
    },
    "id": "fKGGJO3GQ3in",
    "outputId": "aed82adb-d542-4de6-ade7-c2a4f7aadcc6"
   },
   "outputs": [],
   "source": [
    "# Load model 1\n",
    "\n",
    "model_name_1 = \"lmsys/vicuna-7b-v1.5\"\n",
    "sos_tok_1 = False\n",
    "\n",
    "if \"13b\" in model_name_1 or \"12b\" in model_name_1:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt_1 = ModelAndTokenizer(\n",
    "    model_name_1,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "mt_1.set_hs_patch_hooks = model_to_hook[model_name_1]\n",
    "mt_1.model.eval()\n",
    "mt_1.model.to(mt_1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model 2\n",
    "\n",
    "model_name_2 = \"./stable-vicuna-13b\"\n",
    "model_name_2_ = model_name_2.strip('./')\n",
    "sos_tok_2 = False\n",
    "\n",
    "if \"13b\" in model_name_2 or \"12b\" in model_name_2:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt_2 = ModelAndTokenizer(\n",
    "    model_name_2,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "mt_2.set_hs_patch_hooks = model_to_hook[model_name_2]\n",
    "mt_2.model.eval()\n",
    "mt_2.model.to(mt_2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_dataset = datasets.load_from_disk('./the_pile_deduplicated')\n",
    "pile_dataset = pile_dataset.shuffle(seed=42)\n",
    "print(len(pile_dataset))\n",
    "\n",
    "trn_n = 100000\n",
    "val_n = 2000\n",
    "pile_trn = pile_dataset['text'][:trn_n]\n",
    "pile_val = pile_dataset['text'][trn_n:trn_n+val_n]\n",
    "sentences = [(x, 'train') for x in pile_trn] + [(x, 'validation') for x in pile_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    \n",
    "    inp_1_ = make_inputs(mt_1.tokenizer, [sentence], device=mt_1.device)\n",
    "    inp_2_ = make_inputs(mt_2.tokenizer, [sentence], device=mt_2.device)\n",
    "    position = None\n",
    "    k = 0\n",
    "    while k<10:\n",
    "        position_tmp = random.randint(\n",
    "            0, min(max_len - 1, \n",
    "                   len(inp_1_['input_ids'][0]) - 1, \n",
    "                   len(inp_2_['input_ids'][0]) - 1)\n",
    "        )\n",
    "        # cut the tokenized input at the sampled position and turn it back into a string.\n",
    "        # add some buffer at the end such that the tokenization is not modified around the sampled position.\n",
    "        prefix_1 = mt_1.tokenizer.decode(inp_1_['input_ids'][0][:position_tmp + int(sos_tok_1) + 5])\n",
    "        prefix_2 = mt_2.tokenizer.decode(inp_2_['input_ids'][0][:position_tmp + int(sos_tok_2) + 5])\n",
    "        \n",
    "        # check that the selected position corresponds to the same part of the string by \n",
    "        # comparing the prefixes until the sampled position. also make sure that this re-tokenization\n",
    "        # does not shift the sampled position off the sequence length.\n",
    "        inp_1 = make_inputs(mt_1.tokenizer, [prefix_1], device=mt_1.device)\n",
    "        inp_2 = make_inputs(mt_2.tokenizer, [prefix_2], device=mt_2.device)\n",
    "        if prefix_1 == prefix_2 and position_tmp < min(len(inp_1['input_ids'][0]), \n",
    "                                                       len(inp_2['input_ids'][0])):\n",
    "            position = position_tmp\n",
    "            break\n",
    "        k += 1\n",
    "    if position is None:\n",
    "        continue\n",
    "    \n",
    "    for mt, model_name, inp, sos_tok in zip(\n",
    "        [mt_1, mt_2],\n",
    "        [model_name_1, model_name_2],\n",
    "        [inp_1, inp_2],\n",
    "        [sos_tok_1, sos_tok_2]\n",
    "    ):\n",
    "        position_ = position + int(sos_tok)\n",
    "        if (prefix_1, position_, split, model_name) not in data:\n",
    "            output = mt.model(**inp, output_hidden_states = True)\n",
    "\n",
    "            data[(prefix_1, position_, split, model_name)] =  [\n",
    "                output[\"hidden_states\"][layer+1][0][position_].detach().cpu().numpy()\n",
    "                for layer in range(mt.num_layers)\n",
    "            ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'model_name', 'hidden_rep']   \n",
    "\n",
    "for model_name in [model_name_1, model_name_2]:\n",
    "    df[df['model_name'] == model_name].to_pickle(f\"{model_name}_pile_trn_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and unpad \n",
    "\n",
    "pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "unpad = lambda x: x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sources = [l for l in range(0, mt_1.num_layers, 5)]\n",
    "layer_targets = [l for l in range(0, mt_2.num_layers, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{model_name_1}_{model_name_2_}_mappings_pile'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "df_trn_1 = pd.DataFrame(df[(df['data_split'] == 'train') & \n",
    "                           (df['model_name'] == model_name_1)]['hidden_rep'].to_list(), \n",
    "                        columns=[layer for layer in range(mt_1.num_layers)])\n",
    "df_trn_2 = pd.DataFrame(df[(df['data_split'] == 'train') & \n",
    "                           (df['model_name'] == model_name_2)]['hidden_rep'].to_list(), \n",
    "                        columns=[layer for layer in range(mt_2.num_layers)])\n",
    "\n",
    "layer_sources = [l for l in range(0, mt_1.num_layers, 5)]\n",
    "layer_targets = [l for l in range(0, mt_2.num_layers, 5)]\n",
    "\n",
    "mappings = {}\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in layer_targets:\n",
    "        X = np.array(\n",
    "            df_trn_1[layer_source].values.tolist()\n",
    "        )\n",
    "        Y = np.array(\n",
    "            df_trn_2[layer_target].values.tolist()\n",
    "        )\n",
    "\n",
    "        # Solve the least squares problem X * A = Y\n",
    "        # to find our transformation matrix A\n",
    "        A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "        transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "        mappings[(layer_source, layer_target)] = A\n",
    "        with open(f'{model_name_1}_{model_name_2_}_mappings_pile/mapping_{layer_source}-{layer_target}.npy', 'wb') as fd:\n",
    "            np.save(fd, A)\n",
    "\n",
    "        print(layer_source, layer_target, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {}\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in layer_targets:\n",
    "        with open(f'{model_name_1}_{model_name_2_}_mappings_pile/mapping_{layer_source}-{layer_target}.npy', 'rb') as fd:\n",
    "            A = np.load(fd)\n",
    "        mappings[(layer_source, layer_target)] = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize validation set\n",
    "\n",
    "df_val = df[(df['data_split'] == 'validation')].groupby(['full_text', 'data_split']).agg(pd.Series.tolist).reset_index()\n",
    "cols = ['position', 'model_name', 'hidden_rep']\n",
    "for col in cols:\n",
    "    df_val[[f'{col}_1', f'{col}_2']] = df_val[col].to_list()\n",
    "\n",
    "df_val = df_val[[col for col in df_val.columns if col not in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate linear mappings on the validation set of WikiText/a sample from the Pile\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        A = mappings[(layer_source, layer_target)]\n",
    "        transform = lambda x: torch.tensor(\n",
    "            np.squeeze(\n",
    "                unpad(np.dot(\n",
    "                    pad(np.expand_dims(x.detach().cpu().numpy(), 0)), \n",
    "                    A\n",
    "                ))\n",
    "            )\n",
    "        ).to(mt_2.device)\n",
    "\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            position_target = row['position_2']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt, prompt, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target, transform=transform)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_mappings_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulted heatmap\n",
    "metric = 'prec_1'\n",
    "tmp = results[['layer_source', 'layer_target', metric]].groupby(['layer_source', 'layer_target']).agg(\"mean\").reset_index()\n",
    "tmp = tmp.pivot(index='layer_source', columns='layer_target', values=metric)\n",
    "\n",
    "sns.heatmap(tmp, annot=True, fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate identity mapping on the validation set of WikiText\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            position_target = row['position_2']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt, prompt, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_identity_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ID prompt on the validation set of WikiText\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "position_target = -1\n",
    "\n",
    "records = []\n",
    "for layer_source in tqdm(layer_sources):\n",
    "    for layer_target in tqdm(layer_targets):\n",
    "        for idx, row in df_val.iterrows():\n",
    "            prompt_source = row['full_text']\n",
    "            position_source = row['position_1']\n",
    "            prec_1, surprisal = evaluate_patch_next_token_prediction_x_model(\n",
    "                mt_1, mt_2, prompt_source, prompt_target, layer_source, layer_target,\n",
    "                position_source, position_target, position_prediction=position_target, transform=None)\n",
    "\n",
    "            records.append({'layer_source': layer_source,\n",
    "                            'layer_target': layer_target,\n",
    "                            'position_source': position_source,\n",
    "                            'position_target': position_target,\n",
    "                            'prec_1': prec_1, \n",
    "                            'surprisal': surprisal})\n",
    "        \n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name_1}_{model_name_2_}_prompt-id_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(f'{model_name_1}_{model_name_2_}_identity_pile_eval.csv')\n",
    "results1[\"variant\"] = \"identity\"\n",
    "results2 = pd.read_csv(f'{model_name_1}_{model_name_2_}_mappings_pile_eval.csv')\n",
    "results2[\"variant\"] = \"affine mapping\"\n",
    "results3 = pd.read_csv(f'{model_name_1}_{model_name_2_}_prompt-id_pile_eval.csv')\n",
    "results3[\"variant\"] = \"prompt id\"\n",
    "\n",
    "results = pd.concat([results1, results2, results3], ignore_index=True)\n",
    "\n",
    "for metric in ['prec_1', 'surprisal']:\n",
    "    ax = sns.lineplot(data=results, x='layer', y=metric, hue=\"variant\")\n",
    "    ax.set_title(f\"{model_name_1.strip('./')} --> {model_name_2_}\")\n",
    "    ax.legend_.set_title('')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
